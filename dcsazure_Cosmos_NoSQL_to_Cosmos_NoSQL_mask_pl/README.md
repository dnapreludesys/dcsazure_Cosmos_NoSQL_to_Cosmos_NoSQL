# dcsazure_Cosmos_NoSQL_to_Cosmos_NoSQL_mask_pl
## Delphix Compliance Services (DCS) for Azure - Cosmos NoSQL to Cosmos NoSQL Masking Pipeline

This pipeline will perform masking of your Azure Cosmos DB (NoSQL API) data.

### Prerequisites

1. Configure the hosted metadata database and associated Azure SQL service (version `V2025.01.30.0`+).
2. Configure the DCS for Azure REST service.
3. Configure an Azure Function for exporting Cosmos DB data to Azure Data Lake Storage (ADLS).
4. Configure an Azure Data Lake Storage Gen2 account for staging Cosmos DB data during masking.

### Importing
There are several linked services that will need to be selected in order to perform the masking of your Cosmos NoSQL data.

These linked service types are needed for the following steps:

`Azure Function` (Cosmos to ADLS) – Linked service associated with exporting Cosmos DB data to ADLS. This will be used for the following steps:
* Copy ADLS files to Cosmos (Azure Function activity)

`Azure Data Lake Storage Gen2` (staging) – Linked service associated with the ADLS account used for staging Cosmos DB data. This will be used for the following steps:
* For dcsazure_Cosmos_NoSQL_ADLS_delimited_filter_test_utility_df/Source (dataFlow)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_filter_test_utility_df/Sink (dataFlow)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_container_and_directory_mask_ds (DelimitedText dataset)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_unfiltered_mask_df/Source (dataFlow)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_unfiltered_mask_df/Sink (dataFlow)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_filtered_mask_df/Source (dataFlow)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_filtered_mask_df/Sink (dataFlow)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_copy_df/Source (dataFlow)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_copy_df/Sink (dataFlow)

`Azure SQL` (metadata) – Linked service associated with your hosted metadata store. This will be used for the following steps:
* Check For Conditional Masking (If Condition activity)
* Check For Conditional Masking (If Condition activity)
* Check For Conditional Masking (If Condition activity)
* Check For Conditional Masking (If Condition activity)
* If Copy Via Dataflow (If Condition activity)
* If Copy Via Dataflow (If Condition activity)
* If Copy Via Dataflow (If Condition activity)
* If Copy Via Dataflow (If Condition activity)
* Check If We Should Reapply Mapping (If Condition activity)
* Configure Masked Status (Script activity)
* dcsazure_Cosmos_NoSQL_ADLS_delimited_metadata_mask_ds (Azure SQL Database dataset)

`REST` (DCS for Azure) – Linked service associated with calling DCS for Azure. This will be used for the following steps:
* dcsazure_Cosmos_NoSQL_ADLS_delimited_mask_df (dataFlow)

### How It Works

* Execute ADLS Masking Pipeline
  * Check If We Should Reapply Mapping
    * If we should, Mark Table Mapping Incomplete. This is done by updating the metadata store to indicate that tables have not had their mapping applied
  * Select Directories We Should Purge
    * Select sink directories with an incomplete mapping and based on the value of P_TRUNCATE_SINK_BEFORE_WRITE, create a list of directories that we should purge
      * For Each Directory To Purge:
        * Check For The Directory
        * If the directory exists, delete everything in that directory
  * Select Tables Without Required Masking. This is done by querying the metadata store.
    * Filter If Copy Unmask Enabled. This is done by applying a filter based on the value of P_COPY_UNMASKED_TABLES
      * For Each Table With No Masking. Provided we have any rows left after applying the filter
        * If Copy Via Dataflow - based on the value of P_COPY_USE_DATAFLOW
          * If the data flow is to be used for copy then call dcsazure_ADLS_to_ADLS_delimited_copy_df
            * Update the mapped status based on the success of this dataflow, and fail accordingly
          * If the data flow is not to be used for copy, then use a copy activity
            * Update the mapped status based on the success of this dataflow, and fail accordingly
  * Select Tables That Require Masking. This is done by querying the metadata store. This will provide a list of tables that need masking, and if they need to be masked leveraging conditional algorithms, the set of required filters.
    * Configure Masked Status. Set the masked status based on the defined filters that need to be applied for the table to be marked as completely mapped.
    * For Each Table To Mask
      * Check if the table must be masked with a filter condition
        * If no filter needs to be applied:
          * Call the dcsazure_ADLS_to_ADLS_delimited_unfiltered_mask_df data flow, passing in parameters as generated by the Lookup Masking Parameters activity
          * Update the mapped status based on the success of this dataflow, and fail accordingly
        * If a filter needs to be applied:
          * Call the dcsazure_ADLS_to_ADLS_delimited_filterd_mask_df data flow, passing in parameters as generated by the Lookup Masking Parameters activity and the filter as determined by the output of For Each Table To Mask
          * Update the mapped status based on the success of this dataflow, and fail accordingly
  * Note that there is a deactivated activity Test Filter Conditions that exists in order to support importing the filter test utility dataflow, this is making it easier to test writing filter conditions leveraging a dataflow debug session
* Cosmos to ADLS
  * Export documents from the Cosmos DB container to ADLS using an Azure Function
* Until Cosmos to ADLS Durable Function is Success
  * Poll the Azure Function execution status until the export completes
* Check Cosmos to ADLS Status
  * Validate that the export completed successfully, otherwise fail the pipeline

### Variables

If you have configured your database using the metadata store scripts, these variables will not need editing. If you
have customized your metadata store, then these variables may need editing.

* `METADATA_SCHEMA` – Schema used for storing metadata (default `dcsazure_metadata_store`)
* `METADATA_RULESET_TABLE` – Table used for storing discovered rulesets (default `discovered_ruleset`)
* `METADATA_SOURCE_TO_SINK_MAPPING_TABLE` – Table defining source-to-sink mappings (default `adf_data_mapping`)
* `METADATA_ADF_TYPE_MAPPING_TABLE` – Table mapping dataset data types to ADF data types (default `adf_type_mapping`)
* `TARGET_BATCH_SIZE` – Target number of rows per batch during masking (default `50000`)
* `DATASET` – Dataset identifier used in the metadata store (default `COSMOS_NOSQL`)
* `CONDITIONAL_MASKING_RESERVED_CHARACTER` – Reserved character used for shorthand column references in conditional masking filters (default `%`)
* `METADATA_EVENT_PROCEDURE_NAME` – Stored procedure used to capture masking execution events and update masking state (default `insert_adf_masking_event`)
* `METADATA_MASKING_PARAMS_PROCEDURE_NAME` – Stored procedure used to generate Cosmos NoSQL masking parameters (default `generate_cosmos_no_sql_masking_parameters`)
* `COLUMN_WIDTH_ESTIMATE` – Estimated column width used for batch size calculation when schema width is unavailable (default `1000`)
* `STORAGE_ACCOUNT` – Azure Data Lake Storage account name used for staging masked data

### Parameters

* `P_COSMOS_SOURCE_DATABASE` – String – Source Cosmos DB database name
* `P_COSMOS_SINK_DATABASE` – String – Target Cosmos DB database name for masked data
* `P_COSMOS_SINK_ENDPOINT` – String – Cosmos DB endpoint URL for the masked target
* `P_COSMOS_SINK_KEY` – SecureString – Cosmos DB access key for the masked target
* `P_COSMOS_CONTAINER` – String – Cosmos DB container name
* `P_ADLS_SOURCE_CONTAINER` – String – ADLS filesystem/container for unmasked data
* `P_ADLS_SINK_CONTAINER` – String – ADLS filesystem/container for masked data
* `P_ADLS_SINK_STORAGE_KEY` – SecureString – ADLS storage account key
* `P_FAIL_ON_NONCONFORMANT_DATA` – Bool – Fail pipeline if non-conformant data is encountered (default `true`)
* `P_COPY_UNMASKED_TABLES` – Bool – Copy data even when no masking rules are defined (default `true`)
* `P_COPY_USE_DATAFLOW` – Bool – Use dataflow instead of copy activity when copying data (default `false`)
* `P_TRUNCATE_SINK_BEFORE_WRITE` – Bool – Truncate target Cosmos container before writing masked data (default `true`)
* `P_REAPPLY_MAPPING` – Bool – Reapply source-to-sink mapping before masking (default `true`)